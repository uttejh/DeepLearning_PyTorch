{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Char-RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ3Odr7QHWLa",
        "colab_type": "text"
      },
      "source": [
        "# Character-Level LSTM in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glNcEOWOJJzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKZfVtpVJFMK",
        "colab_type": "text"
      },
      "source": [
        "**Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxWwTI1uHU8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"anna.txt\", \"r\") as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF9fU9v0YZLW",
        "colab_type": "code",
        "outputId": "ca6fb6ea-1497-4020-980b-8bbfb2769a74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P6Z6BGkZPFK",
        "colab_type": "text"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FocZ_Cy4ZIck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGIfCeFcbHOW",
        "colab_type": "code",
        "outputId": "43b648dc-6adf-42a8-950c-e3b71170e7ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30, 72, 33, 34, 52, 31, 13, 36, 44, 23, 23, 23, 35, 33, 34, 34, 41,\n",
              "       36, 67, 33,  4, 29,  8, 29, 31,  3, 36, 33, 13, 31, 36, 33,  8,  8,\n",
              "       36, 33,  8, 29, 51, 31, 75, 36, 31, 57, 31, 13, 41, 36, 12,  2, 72,\n",
              "       33, 34, 34, 41, 36, 67, 33,  4, 29,  8, 41, 36, 29,  3, 36, 12,  2,\n",
              "       72, 33, 34, 34, 41, 36, 29,  2, 36, 29, 52,  3, 36, 77, 66,  2, 23,\n",
              "       66, 33, 41, 16, 23, 23, 43, 57, 31, 13, 41, 52, 72, 29,  2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xewHMhPxboHR",
        "colab_type": "text"
      },
      "source": [
        "**Preprocess Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uj0xPiybmzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ejGiwgtAXfK",
        "colab_type": "text"
      },
      "source": [
        "**Create MINI-batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksyi9tNKkLPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YebMEcFAZ1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W8vfGBlAcee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "5bbb3f0c-b798-408b-f292-cbc398282087"
      },
      "source": [
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[30 72 33 34 52 31 13 36 44 23]\n",
            " [ 3 77  2 36 52 72 33 52 36 33]\n",
            " [31  2 15 36 77 13 36 33 36 67]\n",
            " [ 3 36 52 72 31 36 10 72 29 31]\n",
            " [36  3 33 66 36 72 31 13 36 52]\n",
            " [10 12  3  3 29 77  2 36 33  2]\n",
            " [36 24  2  2 33 36 72 33 15 36]\n",
            " [73 59  8 77  2  3 51 41 16 36]]\n",
            "\n",
            "y\n",
            " [[72 33 34 52 31 13 36 44 23 23]\n",
            " [77  2 36 52 72 33 52 36 33 52]\n",
            " [ 2 15 36 77 13 36 33 36 67 77]\n",
            " [36 52 72 31 36 10 72 29 31 67]\n",
            " [ 3 33 66 36 72 31 13 36 52 31]\n",
            " [12  3  3 29 77  2 36 33  2 15]\n",
            " [24  2  2 33 36 72 33 15 36  3]\n",
            " [59  8 77  2  3 51 41 16 36 48]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIvFy6vuAmy9",
        "colab_type": "text"
      },
      "source": [
        "**Define network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJkopO9GAev7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6dca020d-e9e1-4e87-d56e-dcbacdfb5d30"
      },
      "source": [
        "\n",
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HioZr3DwAi0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywkoq9QiAsiY",
        "colab_type": "text"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDRtuBuEAuc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svmZZ9NZAyVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a1ca3665-3723-4234-e3de-858b442638b0"
      },
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6k7AQONA0zx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb008eac-4c02-4e38-f3d0-a51824d37dce"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2499... Val Loss: 3.1765\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1431... Val Loss: 3.1328\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1385... Val Loss: 3.1212\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1137... Val Loss: 3.1192\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1383... Val Loss: 3.1168\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1183... Val Loss: 3.1133\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1047... Val Loss: 3.1082\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1082... Val Loss: 3.0943\n",
            "Epoch: 1/20... Step: 90... Loss: 3.0832... Val Loss: 3.0643\n",
            "Epoch: 1/20... Step: 100... Loss: 3.0214... Val Loss: 2.9955\n",
            "Epoch: 1/20... Step: 110... Loss: 2.9326... Val Loss: 2.8908\n",
            "Epoch: 1/20... Step: 120... Loss: 2.8856... Val Loss: 2.8883\n",
            "Epoch: 1/20... Step: 130... Loss: 2.8120... Val Loss: 2.7617\n",
            "Epoch: 2/20... Step: 140... Loss: 2.7264... Val Loss: 2.6831\n",
            "Epoch: 2/20... Step: 150... Loss: 2.6339... Val Loss: 2.5834\n",
            "Epoch: 2/20... Step: 160... Loss: 2.5532... Val Loss: 2.5209\n",
            "Epoch: 2/20... Step: 170... Loss: 2.4848... Val Loss: 2.4741\n",
            "Epoch: 2/20... Step: 180... Loss: 2.4631... Val Loss: 2.4375\n",
            "Epoch: 2/20... Step: 190... Loss: 2.4140... Val Loss: 2.4050\n",
            "Epoch: 2/20... Step: 200... Loss: 2.4121... Val Loss: 2.3810\n",
            "Epoch: 2/20... Step: 210... Loss: 2.3749... Val Loss: 2.3496\n",
            "Epoch: 2/20... Step: 220... Loss: 2.3395... Val Loss: 2.3205\n",
            "Epoch: 2/20... Step: 230... Loss: 2.3214... Val Loss: 2.2940\n",
            "Epoch: 2/20... Step: 240... Loss: 2.3180... Val Loss: 2.2709\n",
            "Epoch: 2/20... Step: 250... Loss: 2.2467... Val Loss: 2.2443\n",
            "Epoch: 2/20... Step: 260... Loss: 2.2216... Val Loss: 2.2212\n",
            "Epoch: 2/20... Step: 270... Loss: 2.2272... Val Loss: 2.2023\n",
            "Epoch: 3/20... Step: 280... Loss: 2.2325... Val Loss: 2.1809\n",
            "Epoch: 3/20... Step: 290... Loss: 2.1958... Val Loss: 2.1582\n",
            "Epoch: 3/20... Step: 300... Loss: 2.1683... Val Loss: 2.1366\n",
            "Epoch: 3/20... Step: 310... Loss: 2.1379... Val Loss: 2.1188\n",
            "Epoch: 3/20... Step: 320... Loss: 2.1138... Val Loss: 2.0956\n",
            "Epoch: 3/20... Step: 330... Loss: 2.0817... Val Loss: 2.0794\n",
            "Epoch: 3/20... Step: 340... Loss: 2.1022... Val Loss: 2.0586\n",
            "Epoch: 3/20... Step: 350... Loss: 2.0874... Val Loss: 2.0426\n",
            "Epoch: 3/20... Step: 360... Loss: 2.0156... Val Loss: 2.0295\n",
            "Epoch: 3/20... Step: 370... Loss: 2.0355... Val Loss: 2.0072\n",
            "Epoch: 3/20... Step: 380... Loss: 2.0218... Val Loss: 1.9909\n",
            "Epoch: 3/20... Step: 390... Loss: 1.9924... Val Loss: 1.9814\n",
            "Epoch: 3/20... Step: 400... Loss: 1.9583... Val Loss: 1.9590\n",
            "Epoch: 3/20... Step: 410... Loss: 1.9749... Val Loss: 1.9436\n",
            "Epoch: 4/20... Step: 420... Loss: 1.9629... Val Loss: 1.9299\n",
            "Epoch: 4/20... Step: 430... Loss: 1.9603... Val Loss: 1.9140\n",
            "Epoch: 4/20... Step: 440... Loss: 1.9287... Val Loss: 1.9079\n",
            "Epoch: 4/20... Step: 450... Loss: 1.8740... Val Loss: 1.8862\n",
            "Epoch: 4/20... Step: 460... Loss: 1.8615... Val Loss: 1.8734\n",
            "Epoch: 4/20... Step: 470... Loss: 1.8934... Val Loss: 1.8675\n",
            "Epoch: 4/20... Step: 480... Loss: 1.8732... Val Loss: 1.8539\n",
            "Epoch: 4/20... Step: 490... Loss: 1.8792... Val Loss: 1.8387\n",
            "Epoch: 4/20... Step: 500... Loss: 1.8739... Val Loss: 1.8321\n",
            "Epoch: 4/20... Step: 510... Loss: 1.8406... Val Loss: 1.8180\n",
            "Epoch: 4/20... Step: 520... Loss: 1.8536... Val Loss: 1.8072\n",
            "Epoch: 4/20... Step: 530... Loss: 1.8146... Val Loss: 1.7967\n",
            "Epoch: 4/20... Step: 540... Loss: 1.7807... Val Loss: 1.7919\n",
            "Epoch: 4/20... Step: 550... Loss: 1.8298... Val Loss: 1.7711\n",
            "Epoch: 5/20... Step: 560... Loss: 1.7974... Val Loss: 1.7642\n",
            "Epoch: 5/20... Step: 570... Loss: 1.7815... Val Loss: 1.7577\n",
            "Epoch: 5/20... Step: 580... Loss: 1.7578... Val Loss: 1.7424\n",
            "Epoch: 5/20... Step: 590... Loss: 1.7519... Val Loss: 1.7353\n",
            "Epoch: 5/20... Step: 600... Loss: 1.7389... Val Loss: 1.7289\n",
            "Epoch: 5/20... Step: 610... Loss: 1.7287... Val Loss: 1.7219\n",
            "Epoch: 5/20... Step: 620... Loss: 1.7323... Val Loss: 1.7108\n",
            "Epoch: 5/20... Step: 630... Loss: 1.7423... Val Loss: 1.7046\n",
            "Epoch: 5/20... Step: 640... Loss: 1.7145... Val Loss: 1.6979\n",
            "Epoch: 5/20... Step: 650... Loss: 1.7062... Val Loss: 1.6911\n",
            "Epoch: 5/20... Step: 660... Loss: 1.6701... Val Loss: 1.6822\n",
            "Epoch: 5/20... Step: 670... Loss: 1.7074... Val Loss: 1.6746\n",
            "Epoch: 5/20... Step: 680... Loss: 1.7037... Val Loss: 1.6666\n",
            "Epoch: 5/20... Step: 690... Loss: 1.6784... Val Loss: 1.6624\n",
            "Epoch: 6/20... Step: 700... Loss: 1.6788... Val Loss: 1.6561\n",
            "Epoch: 6/20... Step: 710... Loss: 1.6674... Val Loss: 1.6483\n",
            "Epoch: 6/20... Step: 720... Loss: 1.6524... Val Loss: 1.6425\n",
            "Epoch: 6/20... Step: 730... Loss: 1.6723... Val Loss: 1.6354\n",
            "Epoch: 6/20... Step: 740... Loss: 1.6352... Val Loss: 1.6312\n",
            "Epoch: 6/20... Step: 750... Loss: 1.6157... Val Loss: 1.6256\n",
            "Epoch: 6/20... Step: 760... Loss: 1.6548... Val Loss: 1.6234\n",
            "Epoch: 6/20... Step: 770... Loss: 1.6347... Val Loss: 1.6161\n",
            "Epoch: 6/20... Step: 780... Loss: 1.6101... Val Loss: 1.6096\n",
            "Epoch: 6/20... Step: 790... Loss: 1.6057... Val Loss: 1.6031\n",
            "Epoch: 6/20... Step: 800... Loss: 1.6162... Val Loss: 1.5977\n",
            "Epoch: 6/20... Step: 810... Loss: 1.6019... Val Loss: 1.5950\n",
            "Epoch: 6/20... Step: 820... Loss: 1.5792... Val Loss: 1.5893\n",
            "Epoch: 6/20... Step: 830... Loss: 1.6132... Val Loss: 1.5849\n",
            "Epoch: 7/20... Step: 840... Loss: 1.5650... Val Loss: 1.5781\n",
            "Epoch: 7/20... Step: 850... Loss: 1.5862... Val Loss: 1.5749\n",
            "Epoch: 7/20... Step: 860... Loss: 1.5708... Val Loss: 1.5686\n",
            "Epoch: 7/20... Step: 870... Loss: 1.5789... Val Loss: 1.5645\n",
            "Epoch: 7/20... Step: 880... Loss: 1.5770... Val Loss: 1.5613\n",
            "Epoch: 7/20... Step: 890... Loss: 1.5834... Val Loss: 1.5582\n",
            "Epoch: 7/20... Step: 900... Loss: 1.5582... Val Loss: 1.5590\n",
            "Epoch: 7/20... Step: 910... Loss: 1.5233... Val Loss: 1.5556\n",
            "Epoch: 7/20... Step: 920... Loss: 1.5582... Val Loss: 1.5501\n",
            "Epoch: 7/20... Step: 930... Loss: 1.5332... Val Loss: 1.5435\n",
            "Epoch: 7/20... Step: 940... Loss: 1.5503... Val Loss: 1.5417\n",
            "Epoch: 7/20... Step: 950... Loss: 1.5478... Val Loss: 1.5364\n",
            "Epoch: 7/20... Step: 960... Loss: 1.5490... Val Loss: 1.5320\n",
            "Epoch: 7/20... Step: 970... Loss: 1.5570... Val Loss: 1.5286\n",
            "Epoch: 8/20... Step: 980... Loss: 1.5172... Val Loss: 1.5279\n",
            "Epoch: 8/20... Step: 990... Loss: 1.5368... Val Loss: 1.5203\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.5232... Val Loss: 1.5184\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.5535... Val Loss: 1.5132\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.5261... Val Loss: 1.5123\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.5104... Val Loss: 1.5058\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.5211... Val Loss: 1.5039\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.5004... Val Loss: 1.5019\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.4972... Val Loss: 1.4994\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.5101... Val Loss: 1.4952\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.5048... Val Loss: 1.4909\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.4852... Val Loss: 1.4885\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.4822... Val Loss: 1.4849\n",
            "Epoch: 8/20... Step: 1110... Loss: 1.4873... Val Loss: 1.4861\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.4960... Val Loss: 1.4794\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.4831... Val Loss: 1.4783\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.4991... Val Loss: 1.4713\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.5045... Val Loss: 1.4737\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.4585... Val Loss: 1.4668\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.4642... Val Loss: 1.4693\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.4545... Val Loss: 1.4684\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.4891... Val Loss: 1.4650\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.4465... Val Loss: 1.4620\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.4567... Val Loss: 1.4562\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.4575... Val Loss: 1.4570\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.4365... Val Loss: 1.4543\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.4342... Val Loss: 1.4512\n",
            "Epoch: 9/20... Step: 1250... Loss: 1.4498... Val Loss: 1.4470\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.4500... Val Loss: 1.4449\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.4494... Val Loss: 1.4416\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.4596... Val Loss: 1.4395\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.4428... Val Loss: 1.4406\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.4457... Val Loss: 1.4374\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.4527... Val Loss: 1.4355\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.4155... Val Loss: 1.4336\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.4210... Val Loss: 1.4323\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.4008... Val Loss: 1.4315\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.3951... Val Loss: 1.4254\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.4086... Val Loss: 1.4265\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.3923... Val Loss: 1.4236\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.4335... Val Loss: 1.4213\n",
            "Epoch: 10/20... Step: 1390... Loss: 1.4433... Val Loss: 1.4215\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.4389... Val Loss: 1.4161\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.4566... Val Loss: 1.4169\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.4432... Val Loss: 1.4109\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.4161... Val Loss: 1.4178\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.4338... Val Loss: 1.4146\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.3610... Val Loss: 1.4132\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.3850... Val Loss: 1.4111\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.3792... Val Loss: 1.4115\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.3965... Val Loss: 1.4074\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.3925... Val Loss: 1.4008\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.3840... Val Loss: 1.4025\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.3561... Val Loss: 1.4017\n",
            "Epoch: 11/20... Step: 1520... Loss: 1.4041... Val Loss: 1.3954\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.4464... Val Loss: 1.3946\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.3987... Val Loss: 1.3907\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.4015... Val Loss: 1.3900\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.4192... Val Loss: 1.3863\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.3659... Val Loss: 1.3948\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.3452... Val Loss: 1.3894\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.3378... Val Loss: 1.3873\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.3581... Val Loss: 1.3863\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.3587... Val Loss: 1.3923\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.3559... Val Loss: 1.3827\n",
            "Epoch: 12/20... Step: 1630... Loss: 1.3739... Val Loss: 1.3764\n",
            "Epoch: 12/20... Step: 1640... Loss: 1.3458... Val Loss: 1.3793\n",
            "Epoch: 12/20... Step: 1650... Loss: 1.3371... Val Loss: 1.3727\n",
            "Epoch: 12/20... Step: 1660... Loss: 1.3851... Val Loss: 1.3704\n",
            "Epoch: 13/20... Step: 1670... Loss: 1.3548... Val Loss: 1.3746\n",
            "Epoch: 13/20... Step: 1680... Loss: 1.3651... Val Loss: 1.3679\n",
            "Epoch: 13/20... Step: 1690... Loss: 1.3512... Val Loss: 1.3666\n",
            "Epoch: 13/20... Step: 1700... Loss: 1.3491... Val Loss: 1.3657\n",
            "Epoch: 13/20... Step: 1710... Loss: 1.3182... Val Loss: 1.3631\n",
            "Epoch: 13/20... Step: 1720... Loss: 1.3304... Val Loss: 1.3643\n",
            "Epoch: 13/20... Step: 1730... Loss: 1.3726... Val Loss: 1.3612\n",
            "Epoch: 13/20... Step: 1740... Loss: 1.3347... Val Loss: 1.3620\n",
            "Epoch: 13/20... Step: 1750... Loss: 1.3064... Val Loss: 1.3647\n",
            "Epoch: 13/20... Step: 1760... Loss: 1.3408... Val Loss: 1.3624\n",
            "Epoch: 13/20... Step: 1770... Loss: 1.3533... Val Loss: 1.3546\n",
            "Epoch: 13/20... Step: 1780... Loss: 1.3261... Val Loss: 1.3599\n",
            "Epoch: 13/20... Step: 1790... Loss: 1.3145... Val Loss: 1.3510\n",
            "Epoch: 13/20... Step: 1800... Loss: 1.3342... Val Loss: 1.3512\n",
            "Epoch: 14/20... Step: 1810... Loss: 1.3504... Val Loss: 1.3570\n",
            "Epoch: 14/20... Step: 1820... Loss: 1.3321... Val Loss: 1.3522\n",
            "Epoch: 14/20... Step: 1830... Loss: 1.3501... Val Loss: 1.3475\n",
            "Epoch: 14/20... Step: 1840... Loss: 1.2963... Val Loss: 1.3481\n",
            "Epoch: 14/20... Step: 1850... Loss: 1.2679... Val Loss: 1.3478\n",
            "Epoch: 14/20... Step: 1860... Loss: 1.3280... Val Loss: 1.3514\n",
            "Epoch: 14/20... Step: 1870... Loss: 1.3420... Val Loss: 1.3415\n",
            "Epoch: 14/20... Step: 1880... Loss: 1.3315... Val Loss: 1.3430\n",
            "Epoch: 14/20... Step: 1890... Loss: 1.3496... Val Loss: 1.3565\n",
            "Epoch: 14/20... Step: 1900... Loss: 1.3213... Val Loss: 1.3468\n",
            "Epoch: 14/20... Step: 1910... Loss: 1.3147... Val Loss: 1.3419\n",
            "Epoch: 14/20... Step: 1920... Loss: 1.3220... Val Loss: 1.3404\n",
            "Epoch: 14/20... Step: 1930... Loss: 1.2823... Val Loss: 1.3383\n",
            "Epoch: 14/20... Step: 1940... Loss: 1.3384... Val Loss: 1.3290\n",
            "Epoch: 15/20... Step: 1950... Loss: 1.3071... Val Loss: 1.3453\n",
            "Epoch: 15/20... Step: 1960... Loss: 1.3117... Val Loss: 1.3341\n",
            "Epoch: 15/20... Step: 1970... Loss: 1.3056... Val Loss: 1.3341\n",
            "Epoch: 15/20... Step: 1980... Loss: 1.2956... Val Loss: 1.3424\n",
            "Epoch: 15/20... Step: 1990... Loss: 1.2975... Val Loss: 1.3354\n",
            "Epoch: 15/20... Step: 2000... Loss: 1.2776... Val Loss: 1.3279\n",
            "Epoch: 15/20... Step: 2010... Loss: 1.2977... Val Loss: 1.3290\n",
            "Epoch: 15/20... Step: 2020... Loss: 1.3231... Val Loss: 1.3302\n",
            "Epoch: 15/20... Step: 2030... Loss: 1.2820... Val Loss: 1.3378\n",
            "Epoch: 15/20... Step: 2040... Loss: 1.3057... Val Loss: 1.3345\n",
            "Epoch: 15/20... Step: 2050... Loss: 1.2943... Val Loss: 1.3289\n",
            "Epoch: 15/20... Step: 2060... Loss: 1.2976... Val Loss: 1.3264\n",
            "Epoch: 15/20... Step: 2070... Loss: 1.3017... Val Loss: 1.3267\n",
            "Epoch: 15/20... Step: 2080... Loss: 1.2923... Val Loss: 1.3239\n",
            "Epoch: 16/20... Step: 2090... Loss: 1.3021... Val Loss: 1.3261\n",
            "Epoch: 16/20... Step: 2100... Loss: 1.2898... Val Loss: 1.3202\n",
            "Epoch: 16/20... Step: 2110... Loss: 1.2755... Val Loss: 1.3227\n",
            "Epoch: 16/20... Step: 2120... Loss: 1.2961... Val Loss: 1.3327\n",
            "Epoch: 16/20... Step: 2130... Loss: 1.2692... Val Loss: 1.3298\n",
            "Epoch: 16/20... Step: 2140... Loss: 1.2695... Val Loss: 1.3229\n",
            "Epoch: 16/20... Step: 2150... Loss: 1.3047... Val Loss: 1.3210\n",
            "Epoch: 16/20... Step: 2160... Loss: 1.2713... Val Loss: 1.3206\n",
            "Epoch: 16/20... Step: 2170... Loss: 1.2722... Val Loss: 1.3231\n",
            "Epoch: 16/20... Step: 2180... Loss: 1.2647... Val Loss: 1.3183\n",
            "Epoch: 16/20... Step: 2190... Loss: 1.3045... Val Loss: 1.3154\n",
            "Epoch: 16/20... Step: 2200... Loss: 1.2719... Val Loss: 1.3200\n",
            "Epoch: 16/20... Step: 2210... Loss: 1.2377... Val Loss: 1.3103\n",
            "Epoch: 16/20... Step: 2220... Loss: 1.2807... Val Loss: 1.3158\n",
            "Epoch: 17/20... Step: 2230... Loss: 1.2534... Val Loss: 1.3140\n",
            "Epoch: 17/20... Step: 2240... Loss: 1.2735... Val Loss: 1.3141\n",
            "Epoch: 17/20... Step: 2250... Loss: 1.2545... Val Loss: 1.3197\n",
            "Epoch: 17/20... Step: 2260... Loss: 1.2644... Val Loss: 1.3147\n",
            "Epoch: 17/20... Step: 2270... Loss: 1.2748... Val Loss: 1.3141\n",
            "Epoch: 17/20... Step: 2280... Loss: 1.2867... Val Loss: 1.3086\n",
            "Epoch: 17/20... Step: 2290... Loss: 1.2724... Val Loss: 1.3092\n",
            "Epoch: 17/20... Step: 2300... Loss: 1.2407... Val Loss: 1.3106\n",
            "Epoch: 17/20... Step: 2310... Loss: 1.2549... Val Loss: 1.3156\n",
            "Epoch: 17/20... Step: 2320... Loss: 1.2634... Val Loss: 1.3119\n",
            "Epoch: 17/20... Step: 2330... Loss: 1.2585... Val Loss: 1.3055\n",
            "Epoch: 17/20... Step: 2340... Loss: 1.2641... Val Loss: 1.3050\n",
            "Epoch: 17/20... Step: 2350... Loss: 1.2668... Val Loss: 1.3031\n",
            "Epoch: 17/20... Step: 2360... Loss: 1.2675... Val Loss: 1.3012\n",
            "Epoch: 18/20... Step: 2370... Loss: 1.2469... Val Loss: 1.2989\n",
            "Epoch: 18/20... Step: 2380... Loss: 1.2512... Val Loss: 1.3037\n",
            "Epoch: 18/20... Step: 2390... Loss: 1.2598... Val Loss: 1.3059\n",
            "Epoch: 18/20... Step: 2400... Loss: 1.2678... Val Loss: 1.2998\n",
            "Epoch: 18/20... Step: 2410... Loss: 1.2635... Val Loss: 1.2958\n",
            "Epoch: 18/20... Step: 2420... Loss: 1.2539... Val Loss: 1.2948\n",
            "Epoch: 18/20... Step: 2430... Loss: 1.2643... Val Loss: 1.2940\n",
            "Epoch: 18/20... Step: 2440... Loss: 1.2399... Val Loss: 1.3027\n",
            "Epoch: 18/20... Step: 2450... Loss: 1.2424... Val Loss: 1.3015\n",
            "Epoch: 18/20... Step: 2460... Loss: 1.2553... Val Loss: 1.3052\n",
            "Epoch: 18/20... Step: 2470... Loss: 1.2523... Val Loss: 1.2982\n",
            "Epoch: 18/20... Step: 2480... Loss: 1.2460... Val Loss: 1.3017\n",
            "Epoch: 18/20... Step: 2490... Loss: 1.2321... Val Loss: 1.2971\n",
            "Epoch: 18/20... Step: 2500... Loss: 1.2376... Val Loss: 1.2941\n",
            "Epoch: 19/20... Step: 2510... Loss: 1.2420... Val Loss: 1.2886\n",
            "Epoch: 19/20... Step: 2520... Loss: 1.2564... Val Loss: 1.2914\n",
            "Epoch: 19/20... Step: 2530... Loss: 1.2596... Val Loss: 1.2938\n",
            "Epoch: 19/20... Step: 2540... Loss: 1.2732... Val Loss: 1.2910\n",
            "Epoch: 19/20... Step: 2550... Loss: 1.2313... Val Loss: 1.2934\n",
            "Epoch: 19/20... Step: 2560... Loss: 1.2526... Val Loss: 1.2844\n",
            "Epoch: 19/20... Step: 2570... Loss: 1.2353... Val Loss: 1.2884\n",
            "Epoch: 19/20... Step: 2580... Loss: 1.2644... Val Loss: 1.2861\n",
            "Epoch: 19/20... Step: 2590... Loss: 1.2286... Val Loss: 1.2887\n",
            "Epoch: 19/20... Step: 2600... Loss: 1.2349... Val Loss: 1.2886\n",
            "Epoch: 19/20... Step: 2610... Loss: 1.2425... Val Loss: 1.2873\n",
            "Epoch: 19/20... Step: 2620... Loss: 1.2290... Val Loss: 1.2877\n",
            "Epoch: 19/20... Step: 2630... Loss: 1.2146... Val Loss: 1.2867\n",
            "Epoch: 19/20... Step: 2640... Loss: 1.2344... Val Loss: 1.2814\n",
            "Epoch: 20/20... Step: 2650... Loss: 1.2380... Val Loss: 1.2816\n",
            "Epoch: 20/20... Step: 2660... Loss: 1.2473... Val Loss: 1.2865\n",
            "Epoch: 20/20... Step: 2670... Loss: 1.2495... Val Loss: 1.2840\n",
            "Epoch: 20/20... Step: 2680... Loss: 1.2397... Val Loss: 1.2819\n",
            "Epoch: 20/20... Step: 2690... Loss: 1.2292... Val Loss: 1.2869\n",
            "Epoch: 20/20... Step: 2700... Loss: 1.2352... Val Loss: 1.2782\n",
            "Epoch: 20/20... Step: 2710... Loss: 1.2081... Val Loss: 1.2791\n",
            "Epoch: 20/20... Step: 2720... Loss: 1.2120... Val Loss: 1.2792\n",
            "Epoch: 20/20... Step: 2730... Loss: 1.2012... Val Loss: 1.2850\n",
            "Epoch: 20/20... Step: 2740... Loss: 1.2060... Val Loss: 1.2815\n",
            "Epoch: 20/20... Step: 2750... Loss: 1.2100... Val Loss: 1.2849\n",
            "Epoch: 20/20... Step: 2760... Loss: 1.2090... Val Loss: 1.2888\n",
            "Epoch: 20/20... Step: 2770... Loss: 1.2449... Val Loss: 1.2825\n",
            "Epoch: 20/20... Step: 2780... Loss: 1.2561... Val Loss: 1.2876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMIZuvthBbGf",
        "colab_type": "text"
      },
      "source": [
        "**Tips and Tricks**\n",
        "\n",
        "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Solution.ipynb\n",
        "\n",
        "Monitoring Validation Loss vs. Training Loss\n",
        "If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
        "\n",
        "If your training loss is much lower than validation loss then this means the network might be overfitting. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
        "If your training/validation loss are about equal then your model is underfitting. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
        "Approximate number of parameters\n",
        "The two most important parameters that control the model are n_hidden and n_layers. I would advise that you always use n_layers of either 2/3. The n_hidden can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
        "\n",
        "The number of parameters in your model. This is printed when you start training.\n",
        "The size of your dataset. 1MB file is approximately 1 million characters.\n",
        "These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
        "\n",
        "I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make n_hidden larger.\n",
        "I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
        "Best models strategy\n",
        "The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
        "\n",
        "It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
        "\n",
        "By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXswfCmHE-Ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeeAPssaBs6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZLFVXIPBt11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSvFwmHkBy-1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "fd8f41d5-d5c4-464a-8f98-b17d4832ac2a"
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna's first table, as\n",
            "he caled in. And they could say to her all, the coming out of\n",
            "a\n",
            "chance that had been told her to go into the middle of it. He, as the\n",
            "same tone that she had seemed to him, his face alone to\n",
            "the sofe and his chosit weels to bele an easing and secretary\n",
            "of his brain.\n",
            "\n",
            "\"You know, why think these thit, I shall go again to huming.\"\n",
            "\n",
            "\"What she created him, and told you,\" said Stepan Arkadyevitch, smiling,\n",
            "and the disapproach of his wife, as his sender countess was\n",
            "anything in the faces and the front to such seems and a few words.\n",
            "\n",
            "\"You shall be the morning to be distrocked, but the chall\n",
            "assurt and decision,\" he, she come to his head, \"and I\n",
            "say that he ceased to see him.... You're a man of maming.\"\n",
            "\n",
            "Sometimes thought and all the second strong that has always from\n",
            "this marshal. She was not to get in the contente of the clars\n",
            "into him.\n",
            "\n",
            "She went to step a chance that she was stallowing\n",
            "from the colonel and streak in her server and character of all tears.\n",
            "\n",
            "\"You want before yo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD1QKcf8E_xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}